{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "HZEavEH1nDQB"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "#from numba import jit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "zy9fqKcHnDQF"
      },
      "outputs": [],
      "source": [
        "#TODO: loss of least square regression and binary logistic regression\n",
        "'''\n",
        "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
        "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
        "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
        "'''\n",
        "class leastsquare(object):\n",
        "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
        "    def pred(self,score):\n",
        "        return score\n",
        "\n",
        "    def g(self,true,score):\n",
        "        return score - true\n",
        "\n",
        "    def h(self,true,score):\n",
        "        return np.ones_like(score)\n",
        "\n",
        "class logistic(object):\n",
        "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
        "    def pred(self,score):\n",
        "        return 1 / (1 + np.exp(-score))\n",
        "\n",
        "    def g(self,true,score):\n",
        "        p = self.pred(score)\n",
        "        return p - true\n",
        "\n",
        "    def h(self,true,score):\n",
        "        p = self.pred(score)\n",
        "        return p * (1 - p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: class of Random Forest\n",
        "class RF(object):\n",
        "    '''\n",
        "    Class of Random Forest\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        loss: Loss function for gradient boosting.\n",
        "            'mse' for regression task and 'log' for classfication task.\n",
        "            A child class of the loss class could be passed to implement customized loss.\n",
        "        max_depth: The maximum depth d_max of a tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
        "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
        "        num_trees: Number of trees.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "        n_threads = None, loss = 'mse',\n",
        "        max_depth = 3, min_sample_split = 10,\n",
        "        lamda = 1, gamma = 0,\n",
        "        rf = 0.99, num_trees = 100):\n",
        "\n",
        "        self.n_threads = n_threads\n",
        "        self.loss = loss\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.rf = rf\n",
        "        self.num_trees = num_trees\n",
        "\n",
        "    def fit(self, train, target):\n",
        "        # train is n x m 2d numpy array\n",
        "        # target is n-dim 1d array\n",
        "        #TODO\n",
        "        if self.loss == 'mse':\n",
        "            self.loss = leastsquare()\n",
        "        else:\n",
        "            self.loss = logistic()\n",
        "        self.estimators = []\n",
        "        n = train.shape[0]\n",
        "        for _ in range(self.num_trees):\n",
        "            estimator = DecisionTree(max_depth = self.max_depth,\n",
        "                    min_sample_split = self.min_sample_split, gamma = self.gamma)\n",
        "            indices = np.random.choice(n, n, replace=True)\n",
        "            train_subset, target_subset = train[indices], target[indices]\n",
        "            estimator.fit(train_subset, target_subset)\n",
        "            self.estimators.append(estimator)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
        "        if self.loss == 'mse':\n",
        "            score = np.mean(predictions, axis=0)\n",
        "        else:\n",
        "            score = []\n",
        "            for i in range(predictions.shape[1]):\n",
        "                sample_predictions = predictions[:, i]\n",
        "                binary_predictions = (sample_predictions >= 0.5).astype(int)\n",
        "                max_vote_result = np.bincount(binary_predictions).argmax()\n",
        "                score.append(max_vote_result)\n",
        "        return score"
      ],
      "metadata": {
        "id": "JrfyoFLTmfd-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "WbbZkTNBnDQH"
      },
      "outputs": [],
      "source": [
        "# TODO: class of GBDT\n",
        "class GBDT(object):\n",
        "    '''\n",
        "    Class of gradient boosting decision tree (GBDT)\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        loss: Loss function for gradient boosting.\n",
        "            'mse' for regression task and 'log' for classfication task.\n",
        "            A child class of the loss class could be passed to implement customized loss.\n",
        "        max_depth: The maximum depth D_max of a tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
        "        learning_rate: The learning rate eta of GBDT.\n",
        "        num_trees: Number of trees.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "        n_threads = None, loss = 'mse',\n",
        "        max_depth = 3, min_sample_split = 10,\n",
        "        lamda = 1, gamma = 0,\n",
        "        learning_rate = 0.1, num_trees = 100):\n",
        "\n",
        "        self.n_threads = n_threads\n",
        "        self.loss = loss\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_trees = num_trees\n",
        "\n",
        "    def fit(self, train, target):\n",
        "        # train is n x m 2d numpy array\n",
        "        # target is n-dim 1d array\n",
        "        #TODO\n",
        "        self.estimators=[]\n",
        "        if self.loss=='mse':\n",
        "            self.loss=leastsquare()\n",
        "        if self.loss=='log':\n",
        "            self.loss=logistic()\n",
        "        self.score_start=target.mean()\n",
        "        score=np.ones(len(train))*self.score_start\n",
        "        for i in range(self.num_trees):\n",
        "            estimator=Tree(n_threads=self.n_threads,\n",
        "                           max_depth=self.max_depth,min_sample_split=self.min_sample_split,lamda=self.lamda,gamma=self.gamma)\n",
        "            estimator.fit(train,g=self.loss.g(target,score),h=self.loss.h(target,score))\n",
        "            self.estimators.append(estimator)\n",
        "            score+=self.learning_rate*estimator.predict(train)\n",
        "        return self\n",
        "\n",
        "    def predict(self, test):\n",
        "        #TODO\n",
        "        score=np.ones(len(test))*self.score_start\n",
        "        for i in range(self.num_trees):\n",
        "            score+=self.learning_rate*self.estimators[i].predict(test)\n",
        "        return self.loss.pred(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "collapsed": true,
        "id": "M3PJFkMAnDQH"
      },
      "outputs": [],
      "source": [
        "# TODO: class of a node on a tree\n",
        "class TreeNode(object):\n",
        "    '''\n",
        "    Data structure that are used for storing a node on a tree.\n",
        "\n",
        "    A tree is presented by a set of nested TreeNodes,\n",
        "    with one TreeNode pointing two child TreeNodes,\n",
        "    until a tree leaf is reached.\n",
        "\n",
        "    A node on a tree can be either a leaf node or a non-leaf node.\n",
        "    '''\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, is_leaf = False, score = None,\n",
        "                 split_feature = None, split_threshold = None,\n",
        "                 left_child = None, right_child = None, max_depth = 0):\n",
        "        self.is_leaf = is_leaf\n",
        "        self.score = score\n",
        "        self.split_feature = split_feature\n",
        "        self.split_threshold = split_threshold\n",
        "        self.left_child = left_child\n",
        "        self.right_child = right_child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "collapsed": true,
        "id": "k65RKQSanDQI"
      },
      "outputs": [],
      "source": [
        "# TODO: class of single tree\n",
        "class Tree(object):\n",
        "    '''\n",
        "    Class of a single decision tree in GBDT\n",
        "\n",
        "    Parameters:\n",
        "        n_threads: The number of threads used for fitting and predicting.\n",
        "        max_depth: The maximum depth of the tree.\n",
        "        min_sample_split: The minimum number of samples required to further split a node.\n",
        "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
        "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
        "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
        "            rf = 0 means we are training a GBDT.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_threads = None,\n",
        "                 max_depth = 3, min_sample_split = 10,\n",
        "                 lamda = 1, gamma = 0, rf = 0):\n",
        "        self.n_threads = n_threads\n",
        "        self.max_depth = max_depth\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.lamda = lamda\n",
        "        self.gamma = gamma\n",
        "        self.rf = rf\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, train, g, h):\n",
        "        '''\n",
        "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
        "        g and h are gradient and hessian respectively.\n",
        "        '''\n",
        "        #TODO\n",
        "        self.estimator = self.construct_tree(train, g, h, self.max_depth)\n",
        "        return self\n",
        "\n",
        "    def predict(self,test):\n",
        "        '''\n",
        "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
        "        Return predictions (scores) as an array.\n",
        "        '''\n",
        "        #TODO\n",
        "        pool = Pool(self.n_threads)\n",
        "        f = partial(self.predict_single, self.estimator)\n",
        "        result = np.array(pool.map(f,test))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        return result\n",
        "\n",
        "    def predict_single(self, treenode, test):\n",
        "        if treenode.is_leaf:\n",
        "            return treenode.score\n",
        "        else:\n",
        "            if test[treenode.split_feature] < treenode.split_threshold:\n",
        "                return self.predict_single(treenode.left_child, test)\n",
        "            else:\n",
        "                return self.predict_single(treenode.right_child, test)\n",
        "\n",
        "    def construct_tree(self, train, g, h, max_depth):\n",
        "        '''\n",
        "        Tree construction, which is recursively used to grow a tree.\n",
        "        First we should check if we should stop further splitting.\n",
        "\n",
        "        The stopping conditions include:\n",
        "            1. tree reaches max_depth $d_{max}$\n",
        "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
        "            3. gain <= 0\n",
        "        '''\n",
        "        #TODO\n",
        "        if max_depth == 0 or len(train) < self.min_sample_split:\n",
        "            return TreeNode(is_leaf=True, score=self.leaf_score(g, h))\n",
        "\n",
        "        feature, threshold, gain = self.find_best_decision_rule(train, g, h)\n",
        "\n",
        "        if gain <= self.gamma:\n",
        "            return TreeNode(is_leaf=True, score=self.leaf_score(g, h))\n",
        "\n",
        "        index = train[:, feature] < threshold\n",
        "        left_child = self.construct_tree(train[index], g[index], h[index], max_depth - 1)\n",
        "        right_child = self.construct_tree(train[~index], g[~index], h[~index], max_depth - 1)\n",
        "        return TreeNode(split_feature = feature, split_threshold = threshold,\n",
        "                        left_child = left_child, right_child = right_child)\n",
        "\n",
        "    def leaf_score(self,g,h):\n",
        "        return -np.sum(g) / (np.sum(h) + self.lamda)\n",
        "\n",
        "    def leaf_loss(self, g, h):\n",
        "        return -0.5 * np.square(np.sum(g)) / (np.sum(h) + self.lamda)\n",
        "\n",
        "    def find_best_decision_rule(self, train, g, h):\n",
        "        '''\n",
        "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j,\n",
        "        train is the training data assigned to node j\n",
        "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
        "        g and h should be vectors of the same length as the number of data points in train\n",
        "\n",
        "        for each feature, we find the best threshold by find_threshold(),\n",
        "        a [threshold, best_gain] list is returned for each feature.\n",
        "        Then we select the feature with the largest best_gain,\n",
        "        and return the best decision rule [feature, treshold] together with its gain.\n",
        "        '''\n",
        "        #TODO\n",
        "        pool = Pool(self.n_threads)\n",
        "        f = partial(self.find_threshold, g, h)\n",
        "        thresholds=np.array(pool.map(f, train.T))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        feature = np.argmax(thresholds[:,1], axis = 0)\n",
        "        threshold = thresholds[feature, 0]\n",
        "        gain = thresholds[feature, 1]\n",
        "        return feature, threshold, gain\n",
        "\n",
        "    def find_threshold(self, g, h, train):\n",
        "        '''\n",
        "        Given a particular feature $p_j$,\n",
        "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
        "        '''\n",
        "        #TODO\n",
        "        loss = self.leaf_loss(g, h)\n",
        "        threshold = None\n",
        "        best_gain = 0\n",
        "        unq = np.unique(train)\n",
        "        for i in range(1, len(unq)):\n",
        "            this_threshold = (unq[i - 1] + unq[i]) / 2\n",
        "            index = train < this_threshold\n",
        "            left_loss = self.leaf_loss(g[index], h[index])\n",
        "            right_loss = self.leaf_loss(g[~index], h[~index])\n",
        "            this_gain = loss - left_loss - right_loss\n",
        "            if this_gain > best_gain:\n",
        "                threshold = this_threshold\n",
        "                best_gain = this_gain\n",
        "        return [threshold, best_gain]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
        "        ''' constructor '''\n",
        "\n",
        "        # for decision node\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.var_red = var_red\n",
        "\n",
        "        # for leaf node\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "ukfQFp16DQr6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, min_sample_split=2, max_depth=2, gamma=0):\n",
        "        ''' constructor '''\n",
        "\n",
        "        # initialize the root of the tree\n",
        "        self.root = None\n",
        "\n",
        "        # stopping conditions\n",
        "        self.min_sample_split = min_sample_split\n",
        "        self.max_depth = max_depth\n",
        "        self.gamma = gamma\n",
        "        self.rf = 0.99\n",
        "\n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        ''' recursive function to build the tree '''\n",
        "\n",
        "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
        "        num_samples, num_features = np.shape(X)\n",
        "        best_split = {}\n",
        "        # split until stopping conditions are met\n",
        "        if num_samples>=self.min_sample_split and curr_depth<=self.max_depth:\n",
        "            # find the best split\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"var_red\"]>0:\n",
        "                # recur left\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # recur right\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return decision node\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
        "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
        "\n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        # return leaf node\n",
        "        return Node(value=leaf_value)\n",
        "\n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        ''' function to find the best split '''\n",
        "\n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        max_var_red = -float(\"inf\")\n",
        "        # loop over all the features\n",
        "        num_features_to_select = int(self.rf * num_features)\n",
        "        min_rf_m = int(0.2 * num_features_to_select)\n",
        "        max_rf_m = int(0.5 * num_features_to_select)\n",
        "        self.rf_m = np.random.randint(min_rf_m, max_rf_m + 1)\n",
        "        selected_feature_indices = np.random.choice(num_features, self.rf_m, replace=False)\n",
        "        for feature_index in selected_feature_indices:\n",
        "            feature_values = dataset[:, feature_index]\n",
        "            possible_thresholds = np.unique(feature_values)\n",
        "            for threshold in possible_thresholds:\n",
        "                # get current split\n",
        "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
        "                # check if childs are not null\n",
        "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    # compute information gain\n",
        "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
        "                    # update the best split if needed\n",
        "                    if curr_var_red>max_var_red:\n",
        "                        best_split[\"feature_index\"] = feature_index\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"var_red\"] = curr_var_red\n",
        "                        max_var_red = curr_var_red\n",
        "        # return best split\n",
        "        return best_split\n",
        "\n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        ''' function to split the data '''\n",
        "\n",
        "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
        "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
        "        return dataset_left, dataset_right\n",
        "\n",
        "    def variance_reduction(self, parent, l_child, r_child):\n",
        "        ''' function to compute variance reduction '''\n",
        "\n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
        "        return reduction\n",
        "\n",
        "    def calculate_leaf_value(self, Y):\n",
        "        ''' function to compute leaf node '''\n",
        "\n",
        "        val = np.mean(Y)\n",
        "        return val\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        ''' function to train the tree '''\n",
        "\n",
        "        dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "    def make_prediction(self, x, tree):\n",
        "        ''' function to predict new dataset '''\n",
        "\n",
        "        if tree.value!=None: return tree.value\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val<=tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        ''' function to predict a single data point '''\n",
        "\n",
        "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
        "        return preditions"
      ],
      "metadata": {
        "id": "5NB_j-k2DW57"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "CNgv5GW8nDQJ"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
        "\n",
        "# RMSE\n",
        "def root_mean_square_error(pred, y):\n",
        "    #TODO\n",
        "    squared_errors = (pred - y) ** 2\n",
        "    mean_squared_error = np.mean(squared_errors)\n",
        "    rmse = np.sqrt(mean_squared_error)\n",
        "    return rmse\n",
        "\n",
        "# precision\n",
        "def accuracy(pred, y):\n",
        "    #TODO\n",
        "    correct_predictions = np.sum(pred == y)\n",
        "    total_predictions = len(y)\n",
        "    acc = correct_predictions / total_predictions\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# param_grid = {\n",
        "#     'max_depth': [2, 3, 5, 7, 10],\n",
        "#     'min_sample_split': [1, 5, 10, 20, 25, 30, 40, 50],\n",
        "#     'lamda': [0, 1, 3, 5, 7, 10],\n",
        "#     'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1],\n",
        "#     'num_trees': [5, 10, 20, 25, 30, 40, 50],\n",
        "# }"
      ],
      "metadata": {
        "id": "QrX1DGSJvUK6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJkUI-WqnDQJ"
      },
      "outputs": [],
      "source": [
        "# TODO: GBDT regression on boston house price dataset\n",
        "# Case 1\n",
        "# load data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X_1 = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y_1 = raw_df.values[1::2, 2]\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.3, random_state=8)\n",
        "print(X_1.shape, y_1.shape, X_train_1.shape, y_train_1.shape, X_test_1.shape, y_test_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 RF regression on boston house price dataset\n",
        "\n",
        "# create a RF regressor\n",
        "rf_regressor = RF(\n",
        "    max_depth=3,\n",
        "    min_sample_split=10,\n",
        "    gamma=0)\n",
        "\n",
        "# fit the RF regressor\n",
        "rf_regressor.fit(X_train_1, y_train_1)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred_rf = rf_regressor.predict(X_train_1)\n",
        "y_test_pred_rf = rf_regressor.predict(X_test_1)\n",
        "train_rmse_rf = root_mean_square_error(y_train_pred_rf, y_train_1)\n",
        "test_rmse_rf = root_mean_square_error(y_test_pred_rf, y_test_1)\n",
        "\n",
        "print(f\"Training RMSE: {train_rmse_rf}\")\n",
        "print(f\"Test RMSE: {test_rmse_rf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZkTqcUvT-0L",
        "outputId": "ed3495cb-27ec-40e1-a27b-6385df391298"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 2.5431938298646632\n",
            "Test RMSE: 3.9066052988975035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 GBDT regression on boston house price dataset\n",
        "\n",
        "# create a GBDT regressor\n",
        "gbdt_regressor = GBDT(\n",
        "    max_depth=3,\n",
        "    min_sample_split=10,\n",
        "    lamda=1,\n",
        "    gamma=0,\n",
        "    learning_rate=0.1,\n",
        "    num_trees=100\n",
        ")\n",
        "\n",
        "# fit the GBDT regressor\n",
        "gbdt_regressor.fit(X_train_1, y_train_1)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred_gb_1 = gbdt_regressor.predict(X_train_1)\n",
        "y_test_pred_gb_1 = gbdt_regressor.predict(X_test_1)\n",
        "train_rmse_gb_1 = root_mean_square_error(y_train_pred_gb_1, y_train_1)\n",
        "test_rmse_gb_1 = root_mean_square_error(y_test_pred_gb_1, y_test_1)\n",
        "\n",
        "print(\"Training RMSE:\", train_rmse_gb_1)\n",
        "print(\"Test RMSE:\", test_rmse_gb_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfaoXY6-pBkH",
        "outputId": "6bfaacf7-76be-49c5-e16e-3b204dc5b526"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 1.3697603513671248\n",
            "Test RMSE: 3.4842861345840723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXhU6LwdnDQK"
      },
      "outputs": [],
      "source": [
        "# TODO: GBDT classification on credit-g dataset\n",
        "# Case 2\n",
        "# load data\n",
        "from sklearn.datasets import fetch_openml\n",
        "data = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
        "X_2, y_2 = data[0], data[1]\n",
        "y_2 = np.array(list(map(lambda x: 1 if x == 'good' else 0, y_2)))\n",
        "\n",
        "categorical_columns = []\n",
        "for column in X_2.columns:\n",
        "    if X_2[column].dtype == 'category':\n",
        "        categorical_columns.append(column)\n",
        "X_encoded = np.zeros((X_2.shape[0], 0))\n",
        "for column in categorical_columns:\n",
        "    unique_values = np.unique(X_2[column])\n",
        "    num_unique_values = len(unique_values)\n",
        "    encoded_columns = np.zeros((X_2.shape[0], num_unique_values))\n",
        "    for i, value in enumerate(X_2[column]):\n",
        "        encoded_columns[i, np.where(unique_values == value)] = 1\n",
        "    X_encoded = np.concatenate((X_encoded, encoded_columns), axis=1)\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_encoded, y_2, test_size=0.3, random_state=8)\n",
        "print(X_encoded.shape, y_2.shape, X_train_2.shape, y_train_2.shape, X_test_2.shape, y_test_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 RF classification on credit-g dataset\n",
        "\n",
        "# create a RF classifier\n",
        "rf_classifier_c = RF(\n",
        "    loss='log',\n",
        "    max_depth=10,\n",
        "    min_sample_split=2,\n",
        "    gamma=0)\n",
        "\n",
        "# fit the RF classifier on the training data\n",
        "rf_classifier_c.fit(X_train_2, y_train_2)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred_c = rf_classifier_c.predict(X_train_2)\n",
        "y_test_pred_c = rf_classifier_c.predict(X_test_2)\n",
        "y_train_pred_c = (y_train_pred_c > 0.5).astype(int)\n",
        "y_test_pred_c = (y_test_pred_c > 0.5).astype(int)\n",
        "\n",
        "train_accuracy_c = accuracy(y_train_pred_c, y_train_2)\n",
        "test_accuracy_c = accuracy(y_test_pred_c, y_test_2)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy_c:.2f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_c:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34xVMLytTFtq",
        "outputId": "2fa6fd05-b520-4f2a-cc32-8f8f63a8fdbb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.7414285714285714\n",
            "Test Accuracy: 0.7166666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 GBDT classification on credit-g dataset\n",
        "\n",
        "# create a GBDT classifier\n",
        "gbdt_classifier = GBDT(\n",
        "    max_depth=3,\n",
        "    loss = 'log',\n",
        "    min_sample_split=10,\n",
        "    lamda=1,\n",
        "    gamma=0,\n",
        "    learning_rate=0.1,\n",
        "    num_trees=100\n",
        ")\n",
        "\n",
        "# fit the GBDT classifier on the training data\n",
        "gbdt_classifier.fit(X_train_2, y_train_2)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred = gbdt_classifier.predict(X_train_2)\n",
        "y_test_pred = gbdt_classifier.predict(X_test_2)\n",
        "y_train_pred = (y_train_pred > 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "train_accuracy = accuracy(y_train_pred, y_train_2)\n",
        "test_accuracy = accuracy(y_test_pred, y_test_2)\n",
        "\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsUcn4j1reag",
        "outputId": "a701fc6a-4a9e-4117-a0e0-f4c158831e0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8514285714285714\n",
            "Test Accuracy: 0.7733333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3ljHYIbSnDQK"
      },
      "outputs": [],
      "source": [
        "# TODO: GBDT classification on breast cancer dataset\n",
        "# Case 3\n",
        "# load data\n",
        "from sklearn import datasets\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X_3 = breast_cancer.data\n",
        "y_3 = breast_cancer.target\n",
        "\n",
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.3, random_state=8)\n",
        "print(X_3.shape, y_3.shape, X_train_3.shape, y_train_3.shape, X_test_3.shape, y_test_3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 RF classification on breast cancer dataset\n",
        "\n",
        "# create a RF classifier\n",
        "rf_classifier_b = RF(\n",
        "    loss='log',\n",
        "    max_depth=10,\n",
        "    min_sample_split=2)\n",
        "\n",
        "# fit the RF classifier on the training data\n",
        "rf_classifier_b.fit(X_train_3, y_train_3)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred_b = rf_classifier_b.predict(X_train_3)\n",
        "y_test_pred_b = rf_classifier_b.predict(X_test_3)\n",
        "y_train_pred_b = (y_train_pred_b > 0.5).astype(int)\n",
        "y_test_pred_b = (y_test_pred_b > 0.5).astype(int)\n",
        "\n",
        "train_accuracy_b = accuracy(y_train_pred_b, y_train_3)\n",
        "test_accuracy_b = accuracy(y_test_pred_b, y_test_3)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy_b:.2f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_b:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62xn-F3NSQtZ",
        "outputId": "1df0f7fa-ede9-4db9-c8b4-4326b03b3921"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9673366834170855\n",
            "Test Accuracy: 0.9239766081871345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 GBDT classification on breast cancer dataset\n",
        "\n",
        "# create a GBDT classifier\n",
        "gbdt_classifier = GBDT(\n",
        "    max_depth=3,\n",
        "    loss = 'log',\n",
        "    min_sample_split=10,\n",
        "    lamda=1,\n",
        "    gamma=0,\n",
        "    learning_rate=0.1,\n",
        "    num_trees=100\n",
        ")\n",
        "\n",
        "# fit the GBDT classifier on the training data\n",
        "gbdt_classifier.fit(X_train_3, y_train_3)\n",
        "\n",
        "# predict on the training and testing data\n",
        "y_train_pred = gbdt_classifier.predict(X_train_3)\n",
        "y_test_pred = gbdt_classifier.predict(X_test_3)\n",
        "y_train_pred = (y_train_pred > 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "train_accuracy = accuracy(y_train_pred, y_train_3)\n",
        "test_accuracy = accuracy(y_test_pred, y_test_3)\n",
        "\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "cdhC78CUsUt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c394a728-7cd5-4a4f-9766-8b41cf41c5ea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0\n",
            "Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}